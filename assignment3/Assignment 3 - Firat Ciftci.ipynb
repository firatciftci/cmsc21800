{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Prediction/Modeling\n",
    "\n",
    "### Firat Ciftci\n",
    "\n",
    "Due: Friday, Nov 22, 2019 in class\n",
    "\n",
    "Submission: Complete this notebook and print out the output or electronically submit it.\n",
    "\n",
    "Everything you need to complete is marked with a TODO. For textual questions create a new cell under the question to respond to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Game of Thrones is one of the most watched TV series of all times.  With hundreds of characters and more than 22K sentences, this dataset aims to help you test your text mining skills. The content is pretty simple: the dataset contains each and every sentence said in the serie together with who has said it, the episode and the season. For the time being the dataset includes episodes from Season 1 to Season 7. You can download the dataset here: https://github.com/sjyk/cmsc21800/blob/master/got.csv\n",
    "\n",
    "### Loading the Dataset\n",
    "The first task is to load the dataset into a pandas dataframe and filter relevant rows for this assignment. We only care about the rows for chracters that are present in all 7 of the seasons and speak a sufficient amount. Filter the rows to include only those with the speaker name \"Cersei\", \"Daenerys\", \"Tyrion\", and \"Arya\"--make sure you handle upper-case and lower case properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(filename):\n",
    "    '''\n",
    "    Given a filename return a dataframe \n",
    "        containing the rows.\n",
    "       \n",
    "    Only return those rows with a name:\n",
    "    * \"cersei\"\n",
    "    * \"daenerys\"\n",
    "    * \"tyrion\"\n",
    "    * \"arya\"\n",
    "    '''\n",
    "    df = pd.read_csv(filename, delimiter=\";\")\n",
    "    \n",
    "    return df[df['Name'].isin(['cersei', 'daenerys', 'tyrion', 'arya'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Name</th>\n",
       "      <th>N_serie</th>\n",
       "      <th>N_Season</th>\n",
       "      <th>Emision Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22455</th>\n",
       "      <td>22456</td>\n",
       "      <td>Season 7</td>\n",
       "      <td>the dragon and the wolf</td>\n",
       "      <td>And that will be treason</td>\n",
       "      <td>cersei</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>27/08/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22457</th>\n",
       "      <td>22458</td>\n",
       "      <td>Season 7</td>\n",
       "      <td>the dragon and the wolf</td>\n",
       "      <td>Disobeying your queen</td>\n",
       "      <td>cersei</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>27/08/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22459</th>\n",
       "      <td>22460</td>\n",
       "      <td>Season 7</td>\n",
       "      <td>the dragon and the wolf</td>\n",
       "      <td>I told you no one walks away from me</td>\n",
       "      <td>cersei</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>27/08/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22461</th>\n",
       "      <td>22462</td>\n",
       "      <td>Season 7</td>\n",
       "      <td>the dragon and the wolf</td>\n",
       "      <td>Theres one more yet to come</td>\n",
       "      <td>cersei</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>27/08/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22492</th>\n",
       "      <td>22493</td>\n",
       "      <td>Season 7</td>\n",
       "      <td>the dragon and the wolf</td>\n",
       "      <td>Are you all right?</td>\n",
       "      <td>arya</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>27/08/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22494</th>\n",
       "      <td>22495</td>\n",
       "      <td>Season 7</td>\n",
       "      <td>the dragon and the wolf</td>\n",
       "      <td>You did the right thing</td>\n",
       "      <td>arya</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>27/08/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>22497</td>\n",
       "      <td>Season 7</td>\n",
       "      <td>the dragon and the wolf</td>\n",
       "      <td>Im just the executioner You passed the sentenc...</td>\n",
       "      <td>arya</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>27/08/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>22499</td>\n",
       "      <td>Season 7</td>\n",
       "      <td>the dragon and the wolf</td>\n",
       "      <td>I was never going to be as good a lady as you ...</td>\n",
       "      <td>arya</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>27/08/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>22501</td>\n",
       "      <td>Season 7</td>\n",
       "      <td>the dragon and the wolf</td>\n",
       "      <td>I believe thats the nicest thing youve ever sa...</td>\n",
       "      <td>arya</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>27/08/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>22503</td>\n",
       "      <td>Season 7</td>\n",
       "      <td>the dragon and the wolf</td>\n",
       "      <td>In winter, we must protect ourselves Look afte...</td>\n",
       "      <td>arya</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>27/08/2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Column1    Season                  Episode  \\\n",
       "22455    22456  Season 7  the dragon and the wolf   \n",
       "22457    22458  Season 7  the dragon and the wolf   \n",
       "22459    22460  Season 7  the dragon and the wolf   \n",
       "22461    22462  Season 7  the dragon and the wolf   \n",
       "22492    22493  Season 7  the dragon and the wolf   \n",
       "22494    22495  Season 7  the dragon and the wolf   \n",
       "22496    22497  Season 7  the dragon and the wolf   \n",
       "22498    22499  Season 7  the dragon and the wolf   \n",
       "22500    22501  Season 7  the dragon and the wolf   \n",
       "22502    22503  Season 7  the dragon and the wolf   \n",
       "\n",
       "                                                Sentence    Name  N_serie  \\\n",
       "22455                           And that will be treason  cersei       67   \n",
       "22457                              Disobeying your queen  cersei       67   \n",
       "22459               I told you no one walks away from me  cersei       67   \n",
       "22461                        Theres one more yet to come  cersei       67   \n",
       "22492                                 Are you all right?    arya       67   \n",
       "22494                            You did the right thing    arya       67   \n",
       "22496  Im just the executioner You passed the sentenc...    arya       67   \n",
       "22498  I was never going to be as good a lady as you ...    arya       67   \n",
       "22500  I believe thats the nicest thing youve ever sa...    arya       67   \n",
       "22502  In winter, we must protect ourselves Look afte...    arya       67   \n",
       "\n",
       "       N_Season Emision Date  \n",
       "22455         7   27/08/2017  \n",
       "22457         7   27/08/2017  \n",
       "22459         7   27/08/2017  \n",
       "22461         7   27/08/2017  \n",
       "22492         7   27/08/2017  \n",
       "22494         7   27/08/2017  \n",
       "22496         7   27/08/2017  \n",
       "22498         7   27/08/2017  \n",
       "22500         7   27/08/2017  \n",
       "22502         7   27/08/2017  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('got.csv')\n",
    "\n",
    "df[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Cluster Analysis\n",
    "Next, we will mine this dataset to understand what types of structure exist. In the next task, we will write a featurizer that takes the dataset and converts it into a set of feature vectors. We will use a tf-idf featurizer to do this:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def featurize(quotes):\n",
    "    '''\n",
    "    Takes a set of quotes as input and returns two things: an array of feature vectors \n",
    "        and the featurizer. \n",
    "             \n",
    "    * Use the tfidfvectorizer from sklearn and remove english stopwords and restrict the features to \n",
    "        words that appear in *at most* 20 quotes.\n",
    "    \n",
    "    Return values (returns a tuple!!): \n",
    "        X - a dense numpy array of feature vectors representing the text data.\n",
    "        vectorizer - a TfidfVectorizer object.\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=20)\n",
    "    vector = vectorizer.fit_transform(quotes)\n",
    "    \n",
    "    return vector.todense(), vectorizer\n",
    "\n",
    "df = load_dataset('got.csv')\n",
    "X, vectorizer = featurize(df['Sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the principal components of this featurized dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def compute_pca(features, components=2):\n",
    "    '''\n",
    "    Calculate the first two principal components of the \n",
    "         features that you have. Return the components, \n",
    "         the explained variance, and N-D representation of the\n",
    "         feature vectors.\n",
    "\n",
    "    Return Values (returns a 4-tuple): \n",
    "          * axes (the principal components from .components_)\n",
    "          * Y (the dimensionality reduced data)\n",
    "          * c = explained variance on a range from [0,1]\n",
    "    '''\n",
    "    pca = PCA(n_components=components)\n",
    "    pca.fit(features)\n",
    "    \n",
    "    return pca.components_, pca.transform(features), pca.explained_variance_ratio_\n",
    "\n",
    "# Compute PCA\n",
    "pcs, Y, c = compute_pca(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write code to interpret the PCA components. Write a function that uses the vectorizer to determine the words whose presence or absence is strongest in the PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC 1: Most Positive:  [(0.04217712279957113, 'swear'), (0.042549794141531205, 'loved'), (0.04519834594387526, 'taken'), (0.0528466079583697, 'matter'), (0.0597150037400482, 'try'), (0.08119808479588238, 'belong'), (0.08238751952102706, 'swore'), (0.09601781471628285, 'liar'), (0.11093947939592254, 'fuck'), (0.9596955096692612, 'care')]\n",
      "\n",
      "PC 2: Most Positive:  [(0.034843461405484936, 'pick'), (0.042168033162568404, 'slaves'), (0.043217243122619875, 'mycah'), (0.05341996654600781, 'lies'), (0.06505915486791708, 'expect'), (0.07845933208057161, 'traitor'), (0.07907041923630982, 'coward'), (0.09772809651538666, 'alive'), (0.13206552413225245, 'truth'), (0.95648690824929, 'liar')]\n",
      "\n",
      "PC 3: Most Positive:  [(0.04097726062931132, 'desire'), (0.041290885464794494, 'interrupt'), (0.04551883302118401, 'continue'), (0.04939394703887829, 'husband'), (0.04948127694699012, 'catelyn'), (0.04994668863930759, 'loyalty'), (0.0779784998747717, 'rules'), (0.09792828051353855, 'joke'), (0.11700070540297113, 'doesn'), (0.9234743033906512, 'mean')]\n",
      "\n",
      "Explained Variance:  [0.00303668 0.00280099 0.00250545]\n"
     ]
    }
   ],
   "source": [
    "def top_k(pc, vectorizer, k=10):\n",
    "    '''\n",
    "    Finds the highest (most positive) weighted elements in a pc and \n",
    "        then returns the words that correspond to those elements. \n",
    "             \n",
    "    Exclude all words that are less than 3 letters.\n",
    "\n",
    "    Return Value: A set of k words\n",
    "    '''\n",
    "    weights = [x for x in list(zip(pc, vectorizer.get_feature_names())) \\\n",
    "               if len(x[1]) >= 3]\n",
    "    weights.sort()\n",
    "\n",
    "    return weights[-k:]\n",
    "\n",
    "# Extract each of the pcs\n",
    "pc1, pc2, pc3 = pcs\n",
    "    \n",
    "print(\"PC 1: Most Positive: \", top_k(pc1, vectorizer))\n",
    "print(\"\\nPC 2: Most Positive: \", top_k(pc2, vectorizer))\n",
    "print(\"\\nPC 3: Most Positive: \", top_k(pc3, vectorizer))\n",
    "print(\"\\nExplained Variance: \", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those of you who know the story, you can see the story arcs in the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting The Speaker\n",
    "Now, we will have you predict the speaker from the patterns in the text. The first step is to define a training and a test set. Write the following function that splits the loaded dataset into a training set (80% of the data) and a test set (20% of the data). The partition should be random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "def train_test_split(dataframe):\n",
    "    '''\n",
    "    Write a function that splits the dataset into a \n",
    "        training set and a testing set\n",
    "        \n",
    "    Return values (returns a tuple!) :\n",
    "        - A training set 80% of the data,\n",
    "        - A test set 20% of the data.\n",
    "    '''\n",
    "    # The assignment does not put a limit on importing any tools\n",
    "    # in terms of splitting the data, and I think this is the\n",
    "    # best/most efficient way to handle this issue, so I hope\n",
    "    # that me using an imported function here isn't an issue\n",
    "    train, test = split(dataframe, test_size=0.20)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your task is to build a classifier that will achieve at least 45% accuracy on this dataset\n",
    "To achieve this you will have to manipulate the data and play around with different featurization techniques and modeling choices. First, write a function that \"fits\" a language model, such as TFIDF, to the training dataset. It is up to you to tune the parameters for the vectorizer you choose approriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model(training_quotes):\n",
    "    '''\n",
    "    Write a function that instantiates a vectorizer\n",
    "        (e.g., a TfidfVectorizer), runs fit(), and \n",
    "        returns the vectorizer.\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    vectorizer.fit(training_quotes)\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will write a featurizer that takes in a set of quotes and returns an array of feature vectors using the language model above. You may add whatever additional features you find useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_featurize(quotes, vectorizer):\n",
    "    '''\n",
    "    Takes in a set of quotes and returns\n",
    "        an array of feature vectors using\n",
    "        the language model above.\n",
    "    '''\n",
    "    vector = vectorizer.transform(quotes)\n",
    "    \n",
    "    return vector.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, determine the right machine learning model to use to actually make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        arya       0.62      0.24      0.34       157\n",
      "      cersei       0.60      0.35      0.44       198\n",
      "    daenerys       0.55      0.39      0.46       164\n",
      "      tyrion       0.45      0.80      0.58       296\n",
      "\n",
      "    accuracy                           0.50       815\n",
      "   macro avg       0.56      0.44      0.45       815\n",
      "weighted avg       0.54      0.50      0.48       815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = language_model(train['Sentence'])\n",
    "X = prediction_featurize(train['Sentence'], vectorizer)\n",
    "Y = train['Name']\n",
    "\n",
    "Xtest = prediction_featurize(test['Sentence'], vectorizer)\n",
    "Ytest = test['Name']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(X, Y)\n",
    "pred = clf.predict(Xtest)\n",
    "\n",
    "# Calculate accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Ytest, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
